# -*- coding: utf-8 -*-
"""FeatureFunctions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n_lxyurZQC5WlHie0ntG7BV9PgaxVeD9
"""


import pandas as pd
import numpy as np
import os
import operator 
import re
import sys
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import decomposition, ensemble

from textblob import TextBlob
import pandas, xgboost, numpy, textblob, string
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer
from nltk.stem import PorterStemmer
from collections import Counter
import string
from scipy.sparse import hstack
 
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon')
import preprocessor as p
from sentiment_analysis_spanish import sentiment_analysis
#Preproccessing


# helper function for pre-processing text given a file

def process_file(text):

    all_text = text

    #remove all non-alphanumeric chars
    #all_text = re.sub(r"[^a-zA-Z0-9]", " ", all_text)
    #remove newlines/tabs, etc. so it's easier to match phrases, later
    all_text = re.sub(r"\t", " ", all_text)
    all_text = re.sub(r"\n", " ", all_text)
    all_text = re.sub("  ", " ", all_text)
    all_text = re.sub("   ", " ", all_text)
    all_text = re.sub(r'(@USER)', ' ', all_text)
    all_text = re.sub(r"#(\w+)", ' ', all_text)

    return all_text



def create_text_column(df):
  #create copy to modify
  text_df = df.copy()
    
  #store processed text
  text = []
    
      # for each file (row) in the df, read in the file 
  for row_i in df.index:
    

      filename = df.iloc[row_i]['Text']
      file_text = process_file(str(filename))
      file_text = p.clean(str(filename))
          #append processed text to list
      text.append(file_text)
    
    #add column to the copied dataframe
  text_df['proc_text'] = text
    
  return text_df


 
  #SPanish Sentiment Analyzer
def span_sentiment(sentence):
  sentiment = sentiment_analysis.SentimentAnalysisSpanish()
  score = sentiment.sentiment(str(sentence))
  return score

pos_family = {
    'noun' : ['NN','NNS','NNP','NNPS','NOUN'],
    'pron' : ['PRP','PRP$','WP','WP$','PROPN'],
    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ','AUX','VERB'],
    'adj' :  ['JJ','JJR','JJS'],
    'adv' : ['RB','RBR','RBS','WRB'],
    'adp' : ['ADP'],
    'det' : ['DET']
  
}




  # function to check and get the part of speech tag count of a words in a given sentence
def check_pos_tag(x, flag):
    import spacy
    nlp = spacy.load("es_core_news_md")
    blob = TextBlob(x)
    
    if blob.detect_language() != 'en':
        cnt = 0
        try:
            wiki = textblob.TextBlob(x)
            for tup in wiki.tags:
                ppo = list(tup)[1]
                if ppo in pos_family[flag]:
                    cnt += 1
        except:
            pass
        return cnt
    else:
        cnt = 0
        try:
            wiki = nlp(x)
            for tup in wiki:
                ppo = tup.pos_
                if ppo in pos_family[flag]:
                    cnt += 1
        except:
            pass
        return cnt

 

def features(df1, df2):


  complete_df = df1
  features_df = df2

  sent_score = []


  for row in complete_df["proc_text"]:
      score = span_sentiment(str(row))
  
      value = float(score)
      sent_score.append(value)


##Create feature vector 
  features_df["Sentiment"] = sent_score


  features_df['noun_count'] = complete_df['proc_text'].apply(lambda x: check_pos_tag(x, 'noun')).astype(float)
  features_df['verb_count'] = complete_df['proc_text'].apply(lambda x: check_pos_tag(x, 'verb')).astype(float)
  features_df['adj_count'] = complete_df['proc_text'].apply(lambda x: check_pos_tag(x, 'adj')).astype(float)
  features_df['adv_count'] = complete_df['proc_text'].apply(lambda x: check_pos_tag(x, 'adv')).astype(float)
  features_df['pron_count'] = complete_df['proc_text'].apply(lambda x: check_pos_tag(x, 'pron')).astype(float)

  features_df['char_count'] = complete_df['proc_text'].apply(len).astype(float)
  features_df['word_count'] = complete_df['proc_text'].apply(lambda x: len(x.split())).astype(float)
  features_df['word_density'] = features_df['char_count']/ (features_df['word_count']+1).astype(float)
  #features_df['punctuation_count'] = complete_df['proc_text'].apply(lambda x: len("".join(_ for _ in x if _ in string.punctuation))).astype(float)
  #features_df['upper_case_word_count'] = complete_df['proc_text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()])).astype(float)

  return features_df

def normalized_features(df1,df2):
  norm_features_df = df1
  features_df = df2

## should I normalize for every 10/100/200 words? instead of word count like I used here?
  norm_features_df['noun_count_percent'] = features_df['noun_count']/ (features_df['word_count']+1).astype(float)
  norm_features_df['noun_count_percent'] = features_df['verb_count']/ (features_df['word_count']+1).astype(float)
  norm_features_df['adj_count_percent'] = features_df['adj_count']/ (features_df['word_count']+1).astype(float)
  norm_features_df['adv_count_percent'] = features_df['adv_count']/ (features_df['word_count']+1).astype(float)
  norm_features_df['pro_count_percent'] = features_df['pron_count']/ (features_df['word_count']+1).astype(float)

  norm_features_df['char_count'] = features_df['char_count']/ (features_df['word_count']+1).astype(float)
  norm_features_df['word_density'] = features_df['word_density']/ (features_df['word_count']+1).astype(float)
  norm_features_df['punctuation_count'] = features_df['punctuation_count']/ (features_df['word_count']+1).astype(float)
  norm_features_df['upper_case_word_count'] = features_df['upper_case_word_count']/ (features_df['word_count']+1).astype(float)


  norm_features_df["vader_pos"] = features_df["vader_pos"]
  norm_features_df["vader_neg"] = features_df["vader_neg"]
  norm_features_df["vader_neu"] = features_df["vader_neu"]
  norm_features_df["vader_compound"] = features_df["vader_compound"]

   

  return norm_features_df

