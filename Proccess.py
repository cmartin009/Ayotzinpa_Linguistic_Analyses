# -*- coding: utf-8 -*-
"""FeatureFunctions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n_lxyurZQC5WlHie0ntG7BV9PgaxVeD9
"""


import pandas as pd
import numpy as np
import os
import operator 
import re
import sys
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import decomposition, ensemble

from textblob import TextBlob
import pandas, xgboost, numpy, textblob, string
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer
from nltk.stem import PorterStemmer
from collections import Counter
import string
from scipy.sparse import hstack
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon')
import preprocessor as p
from sentiment_analysis_spanish import sentiment_analysis
from nltk.tokenize.treebank import TreebankWordDetokenizer
#Preproccessing


# helper function for pre-processing text given a file

def process_file(text):

    all_text = text

    #remove all non-alphanumeric chars
    #all_text = re.sub(r"[^a-zA-Z0-9]", " ", all_text)
    #remove newlines/tabs, etc. so it's easier to match phrases, later
    all_text = re.sub(r"\t", " ", all_text)
    all_text = re.sub(r"\n", " ", all_text)
    all_text = re.sub("  ", " ", all_text)
    all_text = re.sub("   ", " ", all_text)
    all_text = re.sub(r'(@USER)', ' ', all_text)
    all_text = re.sub(r"#(\w+)", ' ', all_text)
    

    return all_text



def create_text_column(df):
  #create copy to modify
  text_df = df.copy()
    
  #store processed text
  text = []
  text2 = []
    
      # for each file (row) in the df, read in the file 
  for row_i in df.index:
    

      filename = df.iloc[row_i]['content']
      file_text = process_file(str(filename))
      file_text = p.clean(str(filename))
      text.append(file_text)

      stop_words = set(stopwords.words('spanish'))
      word_tokens = word_tokenize(str(file_text))

      filtered_sentence = [ w for w in word_tokens if not w in stop_words]
      filtered_sentence = []
      for w in word_tokens:
          if w not in stop_words:
              filtered_sentence.append(w)
              file_text =  filtered_sentence
          #append processed text to list
      text2.append(file_text)
    
    #add column to the copied dataframe
  text_df['proc_text'] = text
  text_df['tok_text'] = text2
    
  return text_df
